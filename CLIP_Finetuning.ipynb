{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "from PIL import Image\n",
    "import os\n",
    "import os\n",
    "import glob\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "from transformers import CLIPModel, CLIPTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import text_aug as TA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCSV=pd.read_csv('../Data/2D/2d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "dataCSV=dataCSV.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with small dataset\n",
    "# dataCSV=dataCSV[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCSV=dataCSV.fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCSV['image_type']=dataCSV['image_type'].replace(\"x-ray\",\"X-Ray\")\n",
    "dataCSV['image_type']=dataCSV['image_type'].replace(\"X-ray\",\"X-Ray\")\n",
    "\n",
    "dataCSV['image_type']=dataCSV['image_type'].replace(\"xRay\",\"X-Ray\")\n",
    "dataCSV['image_type']=dataCSV['image_type'].replace(\"Xray\",\"X-Ray\")\n",
    "\n",
    "#remove spaces in the image_type\n",
    "dataCSV['image_type']=dataCSV['image_type'].str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = dataCSV['image_dir'].tolist()\n",
    "regions=dataCSV['region'].tolist()\n",
    "image_types=dataCSV['image_type'].tolist()\n",
    "addition_infos=dataCSV['Additional_Info'].tolist()\n",
    "image_index=dataCSV.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "for i in range(len(image_paths)):\n",
    "    image_type=image_types[i]\n",
    "    addition_info=addition_infos[i]\n",
    "    region=regions[i]\n",
    "\n",
    "    text=region+\" \"+image_type\n",
    "    if addition_info!=\"0\":\n",
    "        text=text+\", \"+addition_info\n",
    "    texts.append(text)\n",
    "\n",
    "dataCSV['text']=texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(set(dataCSV['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the pretrained Model\n",
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze the text projection of the model\n",
    "for param in model.text_projection.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#freeze the text encoder of the model\n",
    "# Freeze the text encoder\n",
    "for param in model.text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_path_to_pixels(file_path):\n",
    "    # Determine file type\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    ext = ext.lower()\n",
    "\n",
    "    # Load image\n",
    "    if ext in ['.jpg', '.jpeg']:\n",
    "        image = Image.open(file_path).convert('RGB')\n",
    "    elif ext == '.png':\n",
    "        image = Image.open(file_path).convert('RGB')  # Convert to 3 channels\n",
    "    elif ext == '.dicom':\n",
    "        dicom_image = pydicom.dcmread(file_path)\n",
    "        image_array = dicom_image.pixel_array\n",
    "        if len(image_array.shape) == 2:  # Single-channel\n",
    "            image_array = np.stack((image_array,)*3, axis=-1)\n",
    "        image = Image.fromarray(image_array)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        try:\n",
    "            image = process_image_path_to_pixels(img_path)\n",
    "        except (IOError, FileNotFoundError):\n",
    "            # print(f\"Error loading image {img_path}, using dummy image instead\")\n",
    "            #return dummy tensor\n",
    "            text='dummy'\n",
    "            image=torch.zeros((3,224,224))\n",
    "\n",
    "\n",
    "        # Process image and text together\n",
    "        processed = self.processor(text=text, images=image, return_tensors=\"pt\", padding=\"max_length\")\n",
    "        processed['input_ids']=processed['input_ids'].squeeze(0)\n",
    "        processed['attention_mask']=processed['attention_mask'].squeeze(0)\n",
    "        processed['pixel_values']=processed['pixel_values'].squeeze(0)\n",
    "        return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=CLIPDataset(image_paths, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader= DataLoader(dataset, batch_size=54, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs=10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infonce_loss(img_embed, text_embed, temperature=0.3):\n",
    "    # img_embed=F.normalize(img_embed,dim=1, p=2)\n",
    "    # text_embed=F.normalize(text_embed,dim=1,p=2)\n",
    "    logits = torch.mm(img_embed, text_embed.t()) / temperature\n",
    "    labels = torch.arange(img_embed.size(0)).to(img_embed.device)\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 2 GPU\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])  # assuming you want to use GPUs 0 and 1\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print unfrozen layers\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name+\" is unfrozen\")\n",
    "    else:\n",
    "        print(name+\" is frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contrastive_loss(image_features, text_features, temperature=0.07):\n",
    "    # Normalize features\n",
    "    image_features = F.normalize(image_features, p=2, dim=-1)\n",
    "    text_features = F.normalize(text_features, p=2, dim=-1)\n",
    "\n",
    "    # Cosine similarity as logits\n",
    "    logits = torch.matmul(image_features, text_features.T) / temperature\n",
    "\n",
    "    # Labels (diagonal elements are positives)\n",
    "    labels = torch.arange(len(logits), device=logits.device)\n",
    "\n",
    "    # Symmetrize the loss\n",
    "    loss_i = F.cross_entropy(logits, labels)\n",
    "    loss_t = F.cross_entropy(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss=0\n",
    "    step=0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #plot the pixel values using matplotlib\n",
    "        # plt.imshow(pixel_values[0].permute(1,2,0).cpu().numpy(),cmap='gray')\n",
    "        # plt.show()\n",
    "        outputs = model(pixel_values=pixel_values, attention_mask=attention_mask, input_ids=input_ids)\n",
    "\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "        loss = contrastive_loss(outputs.image_embeds, outputs.text_embeds)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimizer step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step%1000==0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{step}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "        step+=1\n",
    "        #save the model\n",
    "        if step%100==0:\n",
    "            original_model=model.module\n",
    "            torch.save(original_model.state_dict(), \"clip_model_temp.pth\")\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(dataloader)}\")\n",
    "    #save the model\n",
    "    original_model=model.module\n",
    "    torch.save(original_model.state_dict(), \"clip_model_epoch_\"+str(epoch)+\".pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
